{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oL-N9v1wfSr",
        "outputId": "dba45b38-9cfa-4c11-9eb5-15f67cb83c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sqlite3\n",
        "import pickle\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "\n",
        "def scrape_website(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        extracted_text = [paragraph.get_text() for paragraph in paragraphs]\n",
        "\n",
        "        joined_text = \" \".join(extracted_text)\n",
        "        cleaned_text = re.sub(r'\\s+', ' ', joined_text).strip()\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred while scraping the website: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def split_text(text):\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def vectorize_data(sentences):\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_vectors = vectorizer.fit_transform(sentences)\n",
        "        return tfidf_vectors, vectorizer\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during vectorization: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def store_vectors_in_database(tfidf_vectors, sentences):\n",
        "    try:\n",
        "        with sqlite3.connect('vector_database.db') as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''DROP TABLE IF EXISTS tfidf_vectors''')\n",
        "            cursor.execute('''CREATE TABLE IF NOT EXISTS tfidf_vectors (\n",
        "                                id INTEGER PRIMARY KEY,\n",
        "                                sentence TEXT,\n",
        "                                vector BLOB\n",
        "                            )''')\n",
        "            for i, vector in enumerate(tfidf_vectors):\n",
        "                sentence = sentences[i]\n",
        "                vector_serialized = pickle.dumps(vector)\n",
        "                cursor.execute('''INSERT INTO tfidf_vectors (sentence, vector)\n",
        "                                  VALUES (?, ?)''', (sentence, vector_serialized))\n",
        "        st.write(\"TF-IDF vectors stored in database successfully.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred while storing vectors in the database: {e}\")\n",
        "\n",
        "\n",
        "def keyword_matching(question, sentences):\n",
        "    matching_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if re.search(r'\\b{}\\b'.format(re.escape(question)), sentence, re.IGNORECASE):\n",
        "            matching_sentences.append(sentence)\n",
        "    return matching_sentences\n",
        "\n",
        "\n",
        "def semantic_search(question, vectorizer):\n",
        "    try:\n",
        "        question_vector = vectorizer.transform([question])\n",
        "        with sqlite3.connect('vector_database.db') as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute('''SELECT * FROM tfidf_vectors''')\n",
        "            rows = cursor.fetchall()\n",
        "            tfidf_vectors = [pickle.loads(row[2]) for row in rows]\n",
        "\n",
        "        similarities = [cosine_similarity(question_vector, v)[0][0] for v in tfidf_vectors]\n",
        "        max_similarity_index = np.argmax(similarities)\n",
        "        most_similar_sentence = rows[max_similarity_index][1]\n",
        "\n",
        "        return most_similar_sentence\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during semantic search: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def use_llm(question, max_response_length):\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "        inputs = tokenizer.encode(question, return_tensors=\"pt\", max_length=tokenizer.model_max_length, truncation=True)\n",
        "        max_length = min(tokenizer.model_max_length, len(inputs[0]) + max_response_length)\n",
        "\n",
        "        output = model.generate(inputs, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred during GPT-2 response generation: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def give_answer(answer):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Mr-Vicky-01/Bart-Finetuned-conversational-summarization\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Mr-Vicky-01/Bart-Finetuned-conversational-summarization\")\n",
        "\n",
        "    def generate_summary(answer):\n",
        "        inputs = tokenizer([answer], max_length=1024, return_tensors='pt', truncation=True)\n",
        "        summary_ids = model.generate(inputs['input_ids'], max_new_tokens=100, do_sample=False)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "    summary = generate_summary(answer)\n",
        "    return summary\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Webquery: Chat with Website\", page_icon=\":books:\")\n",
        "st.title(\"Webquery: Chat with Website\")\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content=\"Hello, I am a bot. How can I help you?\"),\n",
        "    ]\n",
        "\n",
        "if \"sentences\" not in st.session_state:\n",
        "    st.session_state.sentences = []\n",
        "\n",
        "if \"vectorizer\" not in st.session_state:\n",
        "    st.session_state.vectorizer = None\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Website URL !!!\")\n",
        "    website_url = st.text_input(\"Enter the website URL .......\")\n",
        "    submit = st.button(\"Submit\")\n",
        "\n",
        "if submit:\n",
        "    try:\n",
        "        if os.path.exists('vector_database.db'):\n",
        "            os.remove('vector_database.db')\n",
        "\n",
        "        text = scrape_website(website_url)\n",
        "        st.session_state.sentences = split_text(text)\n",
        "        tfidf_vectors, st.session_state.vectorizer = vectorize_data(st.session_state.sentences)\n",
        "        store_vectors_in_database(tfidf_vectors, st.session_state.sentences)\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "user_query = st.text_input(\"Type your question here....\")\n",
        "\n",
        "if user_query and user_query.strip() and st.session_state.vectorizer:\n",
        "    matching_sentences = keyword_matching(user_query, st.session_state.sentences)\n",
        "    most_similar_sentence = semantic_search(user_query, st.session_state.vectorizer)\n",
        "\n",
        "    if most_similar_sentence:\n",
        "        response = use_llm(most_similar_sentence, max_response_length=len(user_query.split()) * 5)\n",
        "        answer = give_answer(response)\n",
        "        st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
        "        st.session_state.chat_history.append(AIMessage(content=answer))\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ysirH5wh0F",
        "outputId": "3602f8ee-95ae-405f-e2af-bbd622bf1e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 4.587s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLv8t09Kwh1M",
        "outputId": "85253c47-6699-41bb-b100-3c9e884317d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.243.173.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WaSA4BXwh4E",
        "outputId": "d1be6182-2c0a-4048-c3fe-56b8c1c9e8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.968s\n",
            "your url is: https://honest-doors-stick.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_3KXIrZwh6G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}